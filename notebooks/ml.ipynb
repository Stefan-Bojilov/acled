{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dagster_pipeline.dagster_acled.resources.resources import ResourceConfig\n",
    "from dagster_pipeline.dagster_acled.secrets_config import SecretManager\n",
    "\n",
    "import os\n",
    "\n",
    "sm = SecretManager(region_name=os.environ['REGION_NAME'])\n",
    "resources = ResourceConfig.from_secrets(sm = sm, s3_secret_name=\"acled_bucket\", pg_secret_name=\"acled_postgres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = resources['postgres'].get_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rs/lp_7j4vj5_s59_pr61yffxl80000gn/T/ipykernel_51987/259727756.py:5: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  quality_df = pd.read_sql_query(query, conn)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "query = \"SELECT * FROM acled_events_no_delete\"\n",
    "\n",
    "quality_df = pd.read_sql_query(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = quality_df[quality_df['fatalities'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>disorder_type</th>\n",
       "      <th>event_type</th>\n",
       "      <th>sub_event_type</th>\n",
       "      <th>actor1</th>\n",
       "      <th>inter1</th>\n",
       "      <th>actor2</th>\n",
       "      <th>inter2</th>\n",
       "      <th>interaction</th>\n",
       "      <th>admin1</th>\n",
       "      <th>admin2</th>\n",
       "      <th>admin3</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>fatalities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Political violence</td>\n",
       "      <td>Explosions/Remote violence</td>\n",
       "      <td>Air/drone strike</td>\n",
       "      <td>Military Forces of Russia (2000-) Air Force</td>\n",
       "      <td>External/Other forces</td>\n",
       "      <td>Military Forces of Ukraine (2019-)</td>\n",
       "      <td>State forces</td>\n",
       "      <td>State forces-External/Other forces</td>\n",
       "      <td>Zaporizhia</td>\n",
       "      <td>Polohivskyi</td>\n",
       "      <td>Huliaipilska</td>\n",
       "      <td>47.6639</td>\n",
       "      <td>36.2563</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Political violence</td>\n",
       "      <td>Explosions/Remote violence</td>\n",
       "      <td>Air/drone strike</td>\n",
       "      <td>Military Forces of Russia (2000-) Air Force</td>\n",
       "      <td>External/Other forces</td>\n",
       "      <td>Military Forces of Ukraine (2019-)</td>\n",
       "      <td>State forces</td>\n",
       "      <td>State forces-External/Other forces</td>\n",
       "      <td>Donetsk</td>\n",
       "      <td>Kramatorskyi</td>\n",
       "      <td>Kostiantynivska</td>\n",
       "      <td>48.5333</td>\n",
       "      <td>37.7166</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Political violence</td>\n",
       "      <td>Explosions/Remote violence</td>\n",
       "      <td>Air/drone strike</td>\n",
       "      <td>Military Forces of Russia (2000-) Air Force</td>\n",
       "      <td>External/Other forces</td>\n",
       "      <td>Military Forces of Ukraine (2019-)</td>\n",
       "      <td>State forces</td>\n",
       "      <td>State forces-External/Other forces</td>\n",
       "      <td>Donetsk</td>\n",
       "      <td>Pokrovskyi</td>\n",
       "      <td>Hrodivska</td>\n",
       "      <td>48.2986</td>\n",
       "      <td>37.3804</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Political violence</td>\n",
       "      <td>Explosions/Remote violence</td>\n",
       "      <td>Air/drone strike</td>\n",
       "      <td>Military Forces of Russia (2000-) Air Force</td>\n",
       "      <td>External/Other forces</td>\n",
       "      <td>Military Forces of Ukraine (2019-)</td>\n",
       "      <td>State forces</td>\n",
       "      <td>State forces-External/Other forces</td>\n",
       "      <td>Zaporizhia</td>\n",
       "      <td>Vasylivskyi</td>\n",
       "      <td>Stepnohirska</td>\n",
       "      <td>47.5079</td>\n",
       "      <td>35.4622</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Political violence</td>\n",
       "      <td>Explosions/Remote violence</td>\n",
       "      <td>Air/drone strike</td>\n",
       "      <td>Military Forces of Russia (2000-) Air Force</td>\n",
       "      <td>External/Other forces</td>\n",
       "      <td>Military Forces of Ukraine (2019-)</td>\n",
       "      <td>State forces</td>\n",
       "      <td>State forces-External/Other forces</td>\n",
       "      <td>Donetsk</td>\n",
       "      <td>Pokrovskyi</td>\n",
       "      <td>Dobropilska</td>\n",
       "      <td>48.3823</td>\n",
       "      <td>37.1096</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         disorder_type                  event_type    sub_event_type  \\\n",
       "11  Political violence  Explosions/Remote violence  Air/drone strike   \n",
       "16  Political violence  Explosions/Remote violence  Air/drone strike   \n",
       "24  Political violence  Explosions/Remote violence  Air/drone strike   \n",
       "34  Political violence  Explosions/Remote violence  Air/drone strike   \n",
       "36  Political violence  Explosions/Remote violence  Air/drone strike   \n",
       "\n",
       "                                         actor1                 inter1  \\\n",
       "11  Military Forces of Russia (2000-) Air Force  External/Other forces   \n",
       "16  Military Forces of Russia (2000-) Air Force  External/Other forces   \n",
       "24  Military Forces of Russia (2000-) Air Force  External/Other forces   \n",
       "34  Military Forces of Russia (2000-) Air Force  External/Other forces   \n",
       "36  Military Forces of Russia (2000-) Air Force  External/Other forces   \n",
       "\n",
       "                                actor2        inter2  \\\n",
       "11  Military Forces of Ukraine (2019-)  State forces   \n",
       "16  Military Forces of Ukraine (2019-)  State forces   \n",
       "24  Military Forces of Ukraine (2019-)  State forces   \n",
       "34  Military Forces of Ukraine (2019-)  State forces   \n",
       "36  Military Forces of Ukraine (2019-)  State forces   \n",
       "\n",
       "                           interaction      admin1        admin2  \\\n",
       "11  State forces-External/Other forces  Zaporizhia   Polohivskyi   \n",
       "16  State forces-External/Other forces     Donetsk  Kramatorskyi   \n",
       "24  State forces-External/Other forces     Donetsk    Pokrovskyi   \n",
       "34  State forces-External/Other forces  Zaporizhia   Vasylivskyi   \n",
       "36  State forces-External/Other forces     Donetsk    Pokrovskyi   \n",
       "\n",
       "             admin3  latitude  longitude  fatalities  \n",
       "11     Huliaipilska   47.6639    36.2563           2  \n",
       "16  Kostiantynivska   48.5333    37.7166           1  \n",
       "24        Hrodivska   48.2986    37.3804           2  \n",
       "34     Stepnohirska   47.5079    35.4622           1  \n",
       "36      Dobropilska   48.3823    37.1096           2  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['disorder_type', 'event_type', 'sub_event_type', 'actor1', 'inter1', 'actor2', 'inter2', 'interaction', 'admin1', 'admin2', 'admin3', 'latitude', 'longitude', 'fatalities']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disorder_type:\n",
      "disorder_type\n",
      "Political violence        9606\n",
      "Strategic developments       2\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------------------------------------\n",
      "event_type:\n",
      "event_type\n",
      "Battles                       5187\n",
      "Explosions/Remote violence    4407\n",
      "Violence against civilians      12\n",
      "Strategic developments           2\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------------------------------------\n",
      "sub_event_type:\n",
      "sub_event_type\n",
      "Armed clash                            5150\n",
      "Shelling/artillery/missile attack      2979\n",
      "Air/drone strike                       1379\n",
      "Remote explosive/landmine/IED            49\n",
      "Non-state actor overtakes territory      29\n",
      "Attack                                   12\n",
      "Government regains territory              8\n",
      "Disrupted weapons use                     1\n",
      "Other                                     1\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------------------------------------\n",
      "actor1:\n",
      "actor1\n",
      "Military Forces of Russia (2000-)                                        7829\n",
      "Military Forces of Russia (2000-) Air Force                              1250\n",
      "Military Forces of Ukraine (2019-)                                        207\n",
      "Military Forces of Ukraine (2019-) Marines                                129\n",
      "Military Forces of Ukraine (2019-) Air Force                              123\n",
      "Unidentified Armed Group (Ukraine)                                         17\n",
      "Unidentified Military Forces                                               17\n",
      "Police Forces of Ukraine (2019-) State Border Guard Service                 7\n",
      "Military Forces of Ukraine (2019-) Main Directorate of Intelligence         7\n",
      "Military Forces of Ukraine (2019-) National Guard                           6\n",
      "Police Forces of Ukraine (2019-) Security Service of Ukraine                3\n",
      "Military Forces of Russia (2000-) Navy                                      2\n",
      "Military Forces of Russia (2000-) GRU                                       2\n",
      "Berdiansk Communal Militia (Ukraine)                                        2\n",
      "Police Forces of Russia (2000-) Federal Security Service                    2\n",
      "Atesh                                                                       1\n",
      "Melitopol Communal Militia (Ukraine)                                        1\n",
      "LSR: Freedom of Russia Legion                                               1\n",
      "Military Forces of Russia (2000-) Chechen Battalion of Ramzan Kadyrov       1\n",
      "Russian Occupation Government (2022-)                                       1\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------------------------------------\n",
      "inter1:\n",
      "inter1\n",
      "External/Other forces    9103\n",
      "State forces              483\n",
      "Political militia          19\n",
      "Identity militia            3\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------------------------------------\n",
      "actor2:\n",
      "actor2\n",
      "Military Forces of Ukraine (2019-)                                                8332\n",
      "Civilians (Ukraine)                                                                858\n",
      "Military Forces of Russia (2000-)                                                  343\n",
      "Military Forces of Ukraine (2019-) Marines                                          31\n",
      "Military Forces of Ukraine (2019-) Territorial Defense Forces                        7\n",
      "                                                                                     4\n",
      "Police Forces of Ukraine (2019-)                                                     4\n",
      "Military Forces of Ukraine (2019-) Territorial Recruitment Center                    4\n",
      "Military Forces of Ukraine (2019-) Air Force                                         3\n",
      "Civilians (Russia)                                                                   3\n",
      "Military Forces of Russia (2000-) Chechen Battalion of Ramzan Kadyrov                2\n",
      "Police Forces of Ukraine (2019-) State Emergency Service of Ukraine                  2\n",
      "Police Forces of Russia (2000-) Federal Security Service                             2\n",
      "Military Forces of Ukraine (2019-) National Guard                                    2\n",
      "Military Forces of Russia (2000-) Air Force                                          1\n",
      "Military Forces of Ukraine (2019-) Main Directorate of Intelligence                  1\n",
      "Police Forces of Russian Occupation Government (2022-) State Emergency Service       1\n",
      "Civilians (Syria)                                                                    1\n",
      "Skadovsk Communal Militia (Ukraine)                                                  1\n",
      "Police Forces of Russian Occupation Government (2022-)                               1\n",
      "Police Forces of Ukraine (2019-) Security Service of Ukraine                         1\n",
      "Military Forces of Russia (2000-) National Guard                                     1\n",
      "Police Forces of Russia (2000-) State Emergency Service                              1\n",
      "Military Forces of Russia (2000-) Marines                                            1\n",
      "Military Forces of Russia (2000-) 1st Donetsk Army Corps                             1\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------------------------------------\n",
      "inter2:\n",
      "inter2\n",
      "State forces             8389\n",
      "Civilians                 862\n",
      "External/Other forces     352\n",
      "                            4\n",
      "Identity militia            1\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------------------------------------\n",
      "interaction:\n",
      "interaction\n",
      "State forces-External/Other forces             8726\n",
      "External/Other forces-Civilians                 719\n",
      "State forces-Civilians                          132\n",
      "Political militia-Civilians                      10\n",
      "State forces-Political militia                    6\n",
      "State forces-State forces                         3\n",
      "External/Other forces only                        2\n",
      "Identity militia-External/Other forces            2\n",
      "External/Other forces-External/Other forces       2\n",
      "Political militia-External/Other forces           2\n",
      "Political militia only                            1\n",
      "Identity militia-Civilians                        1\n",
      "State forces-Identity militia                     1\n",
      "State forces only                                 1\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------------------------------------\n",
      "admin1:\n",
      "admin1\n",
      "Donetsk            5783\n",
      "Kharkiv            1173\n",
      "Kherson             958\n",
      "Zaporizhia          672\n",
      "Sumy                664\n",
      "Dnipropetrovsk      187\n",
      "Luhansk              67\n",
      "Odesa                27\n",
      "Kyiv City            24\n",
      "Chernihiv            12\n",
      "Kyiv                 11\n",
      "Mykolaiv             11\n",
      "Poltava               5\n",
      "Zhytomyr              4\n",
      "Khmelnytskyi          3\n",
      "Ivano-Frankivsk       2\n",
      "Crimea                2\n",
      "Volyn                 1\n",
      "Chernivtsi            1\n",
      "Rivne                 1\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------------------------------------\n",
      "admin2:\n",
      "admin2\n",
      "Pokrovskyi             2754\n",
      "Kramatorskyi           1294\n",
      "Volnovaskyi             931\n",
      "Khersonskyi             798\n",
      "Bakhmutskyi             748\n",
      "                       ... \n",
      "Berezivskyi               1\n",
      "Shchastynskyi             1\n",
      "Bashtanskyi               1\n",
      "Chernivetskyi             1\n",
      "Kamianets-Podilskyi       1\n",
      "Name: count, Length: 71, dtype: int64\n",
      "--------------------------------------------------------------------------------\n",
      "admin3:\n",
      "admin3\n",
      "Pokrovska              1249\n",
      "Hrodivska               647\n",
      "Khersonska              570\n",
      "Velykonovosilkivska     562\n",
      "Illinivska              436\n",
      "                       ... \n",
      "Alchevska                 1\n",
      "Stepnenska                1\n",
      "Simferopolska             1\n",
      "Kalanchatska              1\n",
      "Yuzhnenska                1\n",
      "Name: count, Length: 236, dtype: int64\n",
      "--------------------------------------------------------------------------------\n",
      "latitude:\n",
      "latitude\n",
      "46.6558    369\n",
      "48.2810    183\n",
      "48.5333    167\n",
      "48.2405    166\n",
      "48.8726    137\n",
      "          ... \n",
      "49.8094      1\n",
      "46.6528      1\n",
      "48.1230      1\n",
      "49.9737      1\n",
      "50.2676      1\n",
      "Name: count, Length: 980, dtype: int64\n",
      "--------------------------------------------------------------------------------\n",
      "longitude:\n",
      "longitude\n",
      "32.6178    369\n",
      "37.1814    183\n",
      "37.7166    167\n",
      "36.9911    166\n",
      "38.0960    137\n",
      "          ... \n",
      "32.1024      1\n",
      "35.3632      1\n",
      "34.3734      1\n",
      "37.3166      1\n",
      "37.4401      1\n",
      "Name: count, Length: 974, dtype: int64\n",
      "--------------------------------------------------------------------------------\n",
      "fatalities:\n",
      "fatalities\n",
      "1      3416\n",
      "2      2736\n",
      "3       794\n",
      "4       322\n",
      "5       316\n",
      "       ... \n",
      "99        1\n",
      "53        1\n",
      "81        1\n",
      "126       1\n",
      "86        1\n",
      "Name: count, Length: 110, dtype: int64\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for col in df.columns: \n",
    "    print(f'{col}:')\n",
    "    print(df[col].value_counts())\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['actor2'] = df['actor2'].replace('', pd.NA)\n",
    "df['inter2'] = df['inter2'].replace('', pd.NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUICK MISSING VALUES CHECK:\n",
      "----------------------------------------\n",
      "actor2: 4 missing (0.0%) - NA: 4, Empty: 0\n",
      "inter2: 4 missing (0.0%) - NA: 4, Empty: 0\n",
      "admin3: 24 missing (0.2%) - NA: 0, Empty: 24\n",
      "\n",
      "Total rows: 9,608\n",
      "================================================================================\n",
      "MISSING VALUES & EMPTY STRING INSPECTION\n",
      "================================================================================\n",
      "Dataset shape: (9608, 14)\n",
      "Total cells: 134,512\n",
      "\n",
      "COLUMN-BY-COLUMN ANALYSIS:\n",
      "--------------------------------------------------\n",
      "\n",
      "📊 Column: disorder_type\n",
      "   Data type: object\n",
      "   Total values: 9,608\n",
      "   Unique values: 2\n",
      "   NA/NaN values: 0\n",
      "   Empty strings (''): 0\n",
      "   Total missing: 0 (0.00%)\n",
      "   ✅ No missing values\n",
      "\n",
      "📊 Column: event_type\n",
      "   Data type: object\n",
      "   Total values: 9,608\n",
      "   Unique values: 4\n",
      "   NA/NaN values: 0\n",
      "   Empty strings (''): 0\n",
      "   Total missing: 0 (0.00%)\n",
      "   ✅ No missing values\n",
      "\n",
      "📊 Column: sub_event_type\n",
      "   Data type: object\n",
      "   Total values: 9,608\n",
      "   Unique values: 9\n",
      "   NA/NaN values: 0\n",
      "   Empty strings (''): 0\n",
      "   Total missing: 0 (0.00%)\n",
      "   ✅ No missing values\n",
      "\n",
      "📊 Column: actor1\n",
      "   Data type: object\n",
      "   Total values: 9,608\n",
      "   Unique values: 20\n",
      "   NA/NaN values: 0\n",
      "   Empty strings (''): 0\n",
      "   Total missing: 0 (0.00%)\n",
      "   ✅ No missing values\n",
      "\n",
      "📊 Column: inter1\n",
      "   Data type: object\n",
      "   Total values: 9,608\n",
      "   Unique values: 4\n",
      "   NA/NaN values: 0\n",
      "   Empty strings (''): 0\n",
      "   Total missing: 0 (0.00%)\n",
      "   ✅ No missing values\n",
      "\n",
      "📊 Column: actor2\n",
      "   Data type: object\n",
      "   Total values: 9,608\n",
      "   Unique values: 24\n",
      "   NA/NaN values: 4\n",
      "   Empty strings (''): 0\n",
      "   Total missing: 4 (0.04%)\n",
      "   ⚠️  HAS MISSING VALUES!\n",
      "   Top values:\n",
      "     Military Forces of Ukraine (2019-): 86.7%\n",
      "     Civilians (Ukraine): 8.9%\n",
      "     Military Forces of Russia (2000-): 3.6%\n",
      "     Military Forces of Ukraine (2019-) Marines: 0.3%\n",
      "     Military Forces of Ukraine (2019-) Territorial Defense Forces: 0.1%\n",
      "\n",
      "📊 Column: inter2\n",
      "   Data type: object\n",
      "   Total values: 9,608\n",
      "   Unique values: 4\n",
      "   NA/NaN values: 4\n",
      "   Empty strings (''): 0\n",
      "   Total missing: 4 (0.04%)\n",
      "   ⚠️  HAS MISSING VALUES!\n",
      "   Top values:\n",
      "     State forces: 87.3%\n",
      "     Civilians: 9.0%\n",
      "     External/Other forces: 3.7%\n",
      "     <NA>: 0.0%\n",
      "     Identity militia: 0.0%\n",
      "\n",
      "📊 Column: interaction\n",
      "   Data type: object\n",
      "   Total values: 9,608\n",
      "   Unique values: 14\n",
      "   NA/NaN values: 0\n",
      "   Empty strings (''): 0\n",
      "   Total missing: 0 (0.00%)\n",
      "   ✅ No missing values\n",
      "\n",
      "📊 Column: admin1\n",
      "   Data type: object\n",
      "   Total values: 9,608\n",
      "   Unique values: 20\n",
      "   NA/NaN values: 0\n",
      "   Empty strings (''): 0\n",
      "   Total missing: 0 (0.00%)\n",
      "   ✅ No missing values\n",
      "\n",
      "📊 Column: admin2\n",
      "   Data type: object\n",
      "   Total values: 9,608\n",
      "   Unique values: 71\n",
      "   NA/NaN values: 0\n",
      "   Empty strings (''): 0\n",
      "   Total missing: 0 (0.00%)\n",
      "   ✅ No missing values\n",
      "\n",
      "📊 Column: admin3\n",
      "   Data type: object\n",
      "   Total values: 9,608\n",
      "   Unique values: 236\n",
      "   NA/NaN values: 0\n",
      "   Empty strings (''): 24\n",
      "   Total missing: 24 (0.25%)\n",
      "   ⚠️  HAS MISSING VALUES!\n",
      "   Top values:\n",
      "     Pokrovska: 13.0%\n",
      "     Hrodivska: 6.7%\n",
      "     Khersonska: 5.9%\n",
      "     Velykonovosilkivska: 5.8%\n",
      "     Illinivska: 4.5%\n",
      "\n",
      "📊 Column: latitude\n",
      "   Data type: float64\n",
      "   Total values: 9,608\n",
      "   Unique values: 980\n",
      "   NA/NaN values: 0\n",
      "   Total missing: 0 (0.00%)\n",
      "   ✅ No missing values\n",
      "\n",
      "📊 Column: longitude\n",
      "   Data type: float64\n",
      "   Total values: 9,608\n",
      "   Unique values: 974\n",
      "   NA/NaN values: 0\n",
      "   Total missing: 0 (0.00%)\n",
      "   ✅ No missing values\n",
      "\n",
      "📊 Column: fatalities\n",
      "   Data type: int64\n",
      "   Total values: 9,608\n",
      "   Unique values: 110\n",
      "   NA/NaN values: 0\n",
      "   Total missing: 0 (0.00%)\n",
      "   ✅ No missing values\n",
      "\n",
      "================================================================================\n",
      "SUMMARY TABLE\n",
      "================================================================================\n",
      "                  dtype total_missing missing_percentage empty_strings  \\\n",
      "admin3           object            24           0.249792            24   \n",
      "actor2           object             4           0.041632             0   \n",
      "inter2           object             4           0.041632             0   \n",
      "disorder_type    object             0                0.0             0   \n",
      "event_type       object             0                0.0             0   \n",
      "sub_event_type   object             0                0.0             0   \n",
      "actor1           object             0                0.0             0   \n",
      "inter1           object             0                0.0             0   \n",
      "interaction      object             0                0.0             0   \n",
      "admin1           object             0                0.0             0   \n",
      "admin2           object             0                0.0             0   \n",
      "latitude        float64             0                0.0             0   \n",
      "longitude       float64             0                0.0             0   \n",
      "fatalities        int64             0                0.0             0   \n",
      "\n",
      "               na_count  \n",
      "admin3                0  \n",
      "actor2                4  \n",
      "inter2                4  \n",
      "disorder_type         0  \n",
      "event_type            0  \n",
      "sub_event_type        0  \n",
      "actor1                0  \n",
      "inter1                0  \n",
      "interaction           0  \n",
      "admin1                0  \n",
      "admin2                0  \n",
      "latitude              0  \n",
      "longitude             0  \n",
      "fatalities            0  \n",
      "\n",
      "📈 OVERALL STATISTICS:\n",
      "------------------------------\n",
      "Total cells in dataset: 134,512\n",
      "Total missing cells: 32\n",
      "Overall missing percentage: 0.02%\n",
      "Columns with missing values: 3 out of 14\n",
      "Problematic columns: actor2, inter2, admin3\n",
      "\n",
      "⚠️  Columns with empty strings: admin3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def inspect_missing_values(df):\n",
    "    \"\"\"\n",
    "    Comprehensive inspection of missing values and empty strings in DataFrame\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"MISSING VALUES & EMPTY STRING INSPECTION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Get basic info about the dataset\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Total cells: {df.shape[0] * df.shape[1]:,}\")\n",
    "    print()\n",
    "    \n",
    "    # Initialize results dictionary\n",
    "    results = {}\n",
    "    \n",
    "    print(\"COLUMN-BY-COLUMN ANALYSIS:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        print(f\"\\n📊 Column: {col}\")\n",
    "        print(f\"   Data type: {df[col].dtype}\")\n",
    "        \n",
    "        # Count different types of missing/empty values\n",
    "        na_count = df[col].isna().sum()\n",
    "        null_count = df[col].isnull().sum()\n",
    "        \n",
    "        # Check for empty strings (only for object/string columns)\n",
    "        if df[col].dtype == 'object':\n",
    "            empty_str_count = (df[col] == '').sum()\n",
    "            whitespace_count = df[col].str.strip().eq('').sum() - empty_str_count\n",
    "            none_str_count = (df[col] == 'None').sum()\n",
    "        else:\n",
    "            empty_str_count = 0\n",
    "            whitespace_count = 0\n",
    "            none_str_count = 0\n",
    "        \n",
    "        # Total missing\n",
    "        total_missing = na_count + empty_str_count + whitespace_count\n",
    "        missing_pct = (total_missing / len(df)) * 100\n",
    "        \n",
    "        # Store results\n",
    "        results[col] = {\n",
    "            'dtype': str(df[col].dtype),\n",
    "            'na_count': na_count,\n",
    "            'empty_strings': empty_str_count,\n",
    "            'whitespace_only': whitespace_count,\n",
    "            'none_strings': none_str_count,\n",
    "            'total_missing': total_missing,\n",
    "            'missing_percentage': missing_pct,\n",
    "            'unique_values': df[col].nunique(),\n",
    "            'total_values': len(df)\n",
    "        }\n",
    "        \n",
    "        # Print summary for this column\n",
    "        print(f\"   Total values: {len(df):,}\")\n",
    "        print(f\"   Unique values: {df[col].nunique():,}\")\n",
    "        print(f\"   NA/NaN values: {na_count:,}\")\n",
    "        \n",
    "        if df[col].dtype == 'object':\n",
    "            print(f\"   Empty strings (''): {empty_str_count:,}\")\n",
    "            if whitespace_count > 0:\n",
    "                print(f\"   Whitespace only: {whitespace_count:,}\")\n",
    "            if none_str_count > 0:\n",
    "                print(f\"   'None' strings: {none_str_count:,}\")\n",
    "        \n",
    "        print(f\"   Total missing: {total_missing:,} ({missing_pct:.2f}%)\")\n",
    "        \n",
    "        # Show examples of missing values if any exist\n",
    "        if total_missing > 0:\n",
    "            print(f\"   ⚠️  HAS MISSING VALUES!\")\n",
    "            \n",
    "            # Show sample of actual values to understand the pattern\n",
    "            if df[col].dtype == 'object':\n",
    "                value_counts = df[col].value_counts(dropna=False, normalize=True).head(5)\n",
    "                print(f\"   Top values:\")\n",
    "                for val, pct in value_counts.items():\n",
    "                    display_val = repr(val) if pd.isna(val) or val == '' else str(val)\n",
    "                    print(f\"     {display_val}: {pct:.1%}\")\n",
    "        else:\n",
    "            print(f\"   ✅ No missing values\")\n",
    "    \n",
    "    # Summary table\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SUMMARY TABLE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    summary_df = pd.DataFrame(results).T\n",
    "    summary_df = summary_df.sort_values('missing_percentage', ascending=False)\n",
    "    \n",
    "    print(summary_df[['dtype', 'total_missing', 'missing_percentage', 'empty_strings', 'na_count']].round(2))\n",
    "    \n",
    "    # Overall statistics\n",
    "    print(f\"\\n📈 OVERALL STATISTICS:\")\n",
    "    print(\"-\" * 30)\n",
    "    total_cells = df.shape[0] * df.shape[1]\n",
    "    total_missing_cells = sum(results[col]['total_missing'] for col in results)\n",
    "    overall_missing_pct = (total_missing_cells / total_cells) * 100\n",
    "    \n",
    "    print(f\"Total cells in dataset: {total_cells:,}\")\n",
    "    print(f\"Total missing cells: {total_missing_cells:,}\")\n",
    "    print(f\"Overall missing percentage: {overall_missing_pct:.2f}%\")\n",
    "    \n",
    "    # Columns with issues\n",
    "    problematic_cols = [col for col in results if results[col]['total_missing'] > 0]\n",
    "    print(f\"Columns with missing values: {len(problematic_cols)} out of {len(df.columns)}\")\n",
    "    \n",
    "    if problematic_cols:\n",
    "        print(f\"Problematic columns: {', '.join(problematic_cols)}\")\n",
    "    \n",
    "    # Special attention to empty strings\n",
    "    empty_string_cols = [col for col in results if results[col]['empty_strings'] > 0]\n",
    "    if empty_string_cols:\n",
    "        print(f\"\\n⚠️  Columns with empty strings: {', '.join(empty_string_cols)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def detailed_empty_string_analysis(df, columns_to_check=None):\n",
    "    \"\"\"\n",
    "    Deep dive into empty string patterns\n",
    "    \"\"\"\n",
    "    if columns_to_check is None:\n",
    "        columns_to_check = df.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DETAILED EMPTY STRING ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for col in columns_to_check:\n",
    "        if (df[col] == '').any():\n",
    "            print(f\"\\n🔍 Column: {col}\")\n",
    "            \n",
    "            # Show context around empty strings\n",
    "            empty_indices = df[df[col] == ''].index[:5]  # First 5 empty string rows\n",
    "            \n",
    "            print(f\"   Rows with empty strings (showing first 5):\")\n",
    "            for idx in empty_indices:\n",
    "                row_data = df.loc[idx, [col] + [c for c in df.columns if c != col][:3]]\n",
    "                print(f\"   Row {idx}: {dict(row_data)}\")\n",
    "\n",
    "# Usage example:\n",
    "# results = inspect_missing_values(df)\n",
    "# detailed_empty_string_analysis(df)\n",
    "\n",
    "# Quick check function for immediate use\n",
    "def quick_missing_check(df):\n",
    "    \"\"\"Quick overview of missing values\"\"\"\n",
    "    print(\"QUICK MISSING VALUES CHECK:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        na_count = df[col].isna().sum()\n",
    "        empty_count = (df[col] == '').sum() if df[col].dtype == 'object' else 0\n",
    "        total_missing = na_count + empty_count\n",
    "        \n",
    "        if total_missing > 0:\n",
    "            pct = (total_missing / len(df)) * 100\n",
    "            print(f\"{col}: {total_missing:,} missing ({pct:.1f}%) - NA: {na_count}, Empty: {empty_count}\")\n",
    "    \n",
    "    print(f\"\\nTotal rows: {len(df):,}\")\n",
    "\n",
    "#Run this on your dataframe:\n",
    "quick_missing_check(df)\n",
    "results = inspect_missing_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CATEGORICAL FEATURE ANALYSIS:\n",
      "--------------------------------------------------\n",
      "\n",
      "disorder_type:\n",
      "  Unique values: 2\n",
      "  Cardinality ratio: 0.000\n",
      "  Target variance: 6.636\n",
      "  Recommended: One-Hot Encoding\n",
      "\n",
      "event_type:\n",
      "  Unique values: 4\n",
      "  Cardinality ratio: 0.000\n",
      "  Target variance: 9.297\n",
      "  Recommended: One-Hot Encoding\n",
      "\n",
      "sub_event_type:\n",
      "  Unique values: 9\n",
      "  Cardinality ratio: 0.001\n",
      "  Target variance: 34.623\n",
      "  Recommended: One-Hot Encoding\n",
      "\n",
      "actor1:\n",
      "  Unique values: 20\n",
      "  Cardinality ratio: 0.002\n",
      "  Target variance: 146.612\n",
      "  Recommended: Label Encoding or Target Encoding\n",
      "\n",
      "inter1:\n",
      "  Unique values: 4\n",
      "  Cardinality ratio: 0.000\n",
      "  Target variance: 160.570\n",
      "  Recommended: One-Hot Encoding\n",
      "\n",
      "actor2:\n",
      "  Unique values: 24\n",
      "  Cardinality ratio: 0.002\n",
      "  Target variance: 133.008\n",
      "  Recommended: Label Encoding or Target Encoding\n",
      "\n",
      "inter2:\n",
      "  Unique values: 4\n",
      "  Cardinality ratio: 0.000\n",
      "  Target variance: 313.236\n",
      "  Recommended: One-Hot Encoding\n",
      "\n",
      "interaction:\n",
      "  Unique values: 14\n",
      "  Cardinality ratio: 0.001\n",
      "  Target variance: 1.608\n",
      "  Recommended: Label Encoding or Target Encoding\n",
      "\n",
      "admin1:\n",
      "  Unique values: 20\n",
      "  Cardinality ratio: 0.002\n",
      "  Target variance: 14.802\n",
      "  Recommended: Label Encoding or Target Encoding\n",
      "\n",
      "admin2:\n",
      "  Unique values: 71\n",
      "  Cardinality ratio: 0.007\n",
      "  Target variance: 8.676\n",
      "  Recommended: Target Encoding (Recommended)\n",
      "\n",
      "admin3:\n",
      "  Unique values: 236\n",
      "  Cardinality ratio: 0.025\n",
      "  Target variance: 12.173\n",
      "  Recommended: Target Encoding (Recommended)\n"
     ]
    }
   ],
   "source": [
    "# Analyze cardinality to choose encoding methods\n",
    "def analyze_categorical_features(df, target_col='fatalities'):\n",
    "    \"\"\"Analyze categorical features to determine best encoding approach\"\"\"\n",
    "    \n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    print(\"CATEGORICAL FEATURE ANALYSIS:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    encoding_strategy = {}\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        unique_count = df[col].nunique()\n",
    "        total_rows = len(df)\n",
    "        cardinality_ratio = unique_count / total_rows\n",
    "        \n",
    "        # Calculate target correlation (for target encoding decision)\n",
    "        grouped = df.groupby(col)[target_col].agg(['mean', 'count'])\n",
    "        target_variance = grouped['mean'].var()\n",
    "        \n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  Unique values: {unique_count}\")\n",
    "        print(f\"  Cardinality ratio: {cardinality_ratio:.3f}\")\n",
    "        print(f\"  Target variance: {target_variance:.3f}\")\n",
    "        \n",
    "        # Determine encoding strategy\n",
    "        if unique_count <= 10:\n",
    "            strategy = \"One-Hot Encoding\"\n",
    "        elif unique_count <= 50 and cardinality_ratio < 0.5:\n",
    "            strategy = \"Label Encoding or Target Encoding\"\n",
    "        elif target_variance > 1.0:  # High target correlation\n",
    "            strategy = \"Target Encoding (Recommended)\"\n",
    "        else:\n",
    "            strategy = \"Frequency/Count Encoding\"\n",
    "            \n",
    "        encoding_strategy[col] = strategy\n",
    "        print(f\"  Recommended: {strategy}\")\n",
    "    \n",
    "    return encoding_strategy\n",
    "\n",
    "# Run this analysis\n",
    "encoding_plan = analyze_categorical_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def target_encode_cv(df, categorical_col, target_col, cv_folds=5, smoothing=10):\n",
    "    \"\"\"\n",
    "    Target encoding with cross-validation and smoothing to prevent overfitting\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "    target_encoded = np.zeros(len(df))\n",
    "    \n",
    "    # Global mean for smoothing and fallback\n",
    "    global_mean = df[target_col].mean()\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(df):\n",
    "        # Calculate target statistics on training fold\n",
    "        train_df = df.iloc[train_idx]\n",
    "        target_stats = train_df.groupby(categorical_col)[target_col].agg(['mean', 'count'])\n",
    "        \n",
    "        # Apply smoothing: (count * category_mean + smoothing * global_mean) / (count + smoothing)\n",
    "        target_stats['smoothed_mean'] = (\n",
    "            (target_stats['count'] * target_stats['mean'] + smoothing * global_mean) /\n",
    "            (target_stats['count'] + smoothing)\n",
    "        )\n",
    "        \n",
    "        # Apply to validation fold\n",
    "        target_encoded[val_idx] = df.iloc[val_idx][categorical_col].map(target_stats['smoothed_mean'])\n",
    "        \n",
    "        # Handle unseen categories with global mean\n",
    "        target_encoded[val_idx] = np.where(\n",
    "            pd.isna(target_encoded[val_idx]), \n",
    "            global_mean, \n",
    "            target_encoded[val_idx]\n",
    "        )\n",
    "    \n",
    "    return target_encoded\n",
    "\n",
    "def prepare_xgboost_features(df, target_col='fatalities'):\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline for XGBoost based on your analysis\n",
    "    \"\"\"\n",
    "    print(\"🚀 Starting XGBoost preprocessing pipeline...\")\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # =============================================================================\n",
    "    # 1. ONE-HOT ENCODING (Low cardinality, high target variance)\n",
    "    # =============================================================================\n",
    "    onehot_cols = [\n",
    "        'disorder_type',    # 2 unique, variance: 6.636\n",
    "        'event_type',       # 4 unique, variance: 9.297  \n",
    "        'sub_event_type',   # 9 unique, variance: 34.623\n",
    "        'inter1',           # 4 unique, variance: 160.570\n",
    "        'inter2'            # 4 unique, variance: 313.236\n",
    "    ]\n",
    "    \n",
    "    print(f\"📊 Applying One-Hot Encoding to: {onehot_cols}\")\n",
    "    df_encoded = pd.get_dummies(df_processed, columns=onehot_cols, prefix=onehot_cols)\n",
    "    print(f\"   Created {len([col for col in df_encoded.columns if any(prefix in col for prefix in onehot_cols)])} binary features\")\n",
    "    \n",
    "    # =============================================================================\n",
    "    # 2. TARGET ENCODING (High cardinality)\n",
    "    # =============================================================================\n",
    "    target_encode_cols = [\n",
    "        'admin2',  # 71 unique, variance: 8.676\n",
    "        'admin3'   # 236 unique, variance: 12.173\n",
    "    ]\n",
    "    \n",
    "    print(f\"🎯 Applying Target Encoding to: {target_encode_cols}\")\n",
    "    for col in target_encode_cols:\n",
    "        encoded_col_name = f'{col}_target_encoded'\n",
    "        df_encoded[encoded_col_name] = target_encode_cv(df_encoded, col, target_col)\n",
    "        print(f\"   ✓ {col} → {encoded_col_name}\")\n",
    "    \n",
    "    # =============================================================================\n",
    "    # 3. DECISION FOR MEDIUM CARDINALITY (Manual choice needed)\n",
    "    # =============================================================================\n",
    "    medium_cardinality_cols = {\n",
    "        'actor1': {'unique': 20, 'variance': 146.612},\n",
    "        'actor2': {'unique': 24, 'variance': 133.008}, \n",
    "        'interaction': {'unique': 14, 'variance': 1.608},\n",
    "        'admin1': {'unique': 20, 'variance': 14.802}\n",
    "    }\n",
    "    \n",
    "    print(f\"⚖️  Processing Medium Cardinality columns:\")\n",
    "    \n",
    "    # HIGH variance → Target Encoding\n",
    "    high_variance_cols = ['actor1', 'actor2']  # variance > 100\n",
    "    for col in high_variance_cols:\n",
    "        encoded_col_name = f'{col}_target_encoded'\n",
    "        df_encoded[encoded_col_name] = target_encode_cv(df_encoded, col, target_col)\n",
    "        print(f\"   🎯 {col} → Target Encoded (high variance: {medium_cardinality_cols[col]['variance']:.1f})\")\n",
    "    \n",
    "    # LOWER variance → Label Encoding (ordinal, preserves some relationship)\n",
    "    lower_variance_cols = ['interaction', 'admin1']  # variance < 20\n",
    "    label_encoders = {}\n",
    "    for col in lower_variance_cols:\n",
    "        le = LabelEncoder()\n",
    "        encoded_col_name = f'{col}_label_encoded'\n",
    "        df_encoded[encoded_col_name] = le.fit_transform(df_encoded[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "        print(f\"   🏷️  {col} → Label Encoded (lower variance: {medium_cardinality_cols[col]['variance']:.1f})\")\n",
    "    \n",
    "    # =============================================================================\n",
    "    # 4. GEOGRAPHIC FEATURE ENGINEERING\n",
    "    # =============================================================================\n",
    "    print(\"🌍 Processing Geographic Features:\")\n",
    "    \n",
    "    # Keep original coordinates\n",
    "    geographic_features = ['latitude', 'longitude']\n",
    "    \n",
    "    # Add derived geographic features\n",
    "    if 'latitude' in df_encoded.columns and 'longitude' in df_encoded.columns:\n",
    "        # Distance from conflict center (approximate center of your data)\n",
    "        center_lat = df_encoded['latitude'].median()\n",
    "        center_lon = df_encoded['longitude'].median()\n",
    "        \n",
    "        df_encoded['distance_from_center'] = np.sqrt(\n",
    "            (df_encoded['latitude'] - center_lat)**2 + \n",
    "            (df_encoded['longitude'] - center_lon)**2\n",
    "        )\n",
    "        \n",
    "        # Regional density (events per geographic area)\n",
    "        df_encoded['lat_rounded'] = df_encoded['latitude'].round(1)\n",
    "        df_encoded['lon_rounded'] = df_encoded['longitude'].round(1)\n",
    "        location_counts = df_encoded.groupby(['lat_rounded', 'lon_rounded']).size()\n",
    "        df_encoded['location_density'] = df_encoded.set_index(['lat_rounded', 'lon_rounded']).index.map(location_counts)\n",
    "        \n",
    "        # Clean up temporary columns\n",
    "        df_encoded.drop(['lat_rounded', 'lon_rounded'], axis=1, inplace=True)\n",
    "        \n",
    "        geographic_features.extend(['distance_from_center', 'location_density'])\n",
    "        print(f\"   ✓ Added: {['distance_from_center', 'location_density']}\")\n",
    "    \n",
    "    # =============================================================================\n",
    "    # 5. PREPARE FINAL FEATURE SET\n",
    "    # =============================================================================\n",
    "    \n",
    "    # Keep numeric target and features\n",
    "    numeric_cols = [target_col] + [col for col in df_encoded.columns if df_encoded[col].dtype in ['int64', 'float64']]\n",
    "    \n",
    "    # Remove original categorical columns that were encoded\n",
    "    original_categorical = [\n",
    "        'disorder_type', 'event_type', 'sub_event_type', 'inter1', 'inter2',\n",
    "        'admin2', 'admin3', 'actor1', 'actor2', 'interaction', 'admin1'\n",
    "    ]\n",
    "    \n",
    "    final_features = [col for col in df_encoded.columns if col not in original_categorical]\n",
    "    df_final = df_encoded[final_features].copy()\n",
    "    \n",
    "    # =============================================================================\n",
    "    # 6. FINAL SUMMARY\n",
    "    # =============================================================================\n",
    "    print(f\"\\n✅ PREPROCESSING COMPLETE!\")\n",
    "    print(f\"📈 Final dataset shape: {df_final.shape}\")\n",
    "    print(f\"🎯 Target column: {target_col}\")\n",
    "    print(f\"🔢 Feature columns: {df_final.shape[1] - 1}\")\n",
    "    \n",
    "    print(f\"\\nFeature types created:\")\n",
    "    onehot_features = len([col for col in df_final.columns if any(prefix in col for prefix in onehot_cols)])\n",
    "    target_features = len([col for col in df_final.columns if 'target_encoded' in col])\n",
    "    label_features = len([col for col in df_final.columns if 'label_encoded' in col])\n",
    "    \n",
    "    print(f\"   One-hot encoded: {onehot_features}\")\n",
    "    print(f\"   Target encoded: {target_features}\")\n",
    "    print(f\"   Label encoded: {label_features}\")\n",
    "    print(f\"   Geographic: {len(geographic_features)}\")\n",
    "    \n",
    "    # Check for any remaining issues\n",
    "    print(f\"\\n🔍 Data quality check:\")\n",
    "    print(f\"   Missing values: {df_final.isnull().sum().sum()}\")\n",
    "    print(f\"   Infinite values: {np.isinf(df_final.select_dtypes(include=[np.number])).sum().sum()}\")\n",
    "    \n",
    "    return df_final, label_encoders\n",
    "\n",
    "def prepare_train_test_split(df_final, target_col='fatalities', test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Prepare features and target for XGBoost training\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df_final.drop(columns=[target_col])\n",
    "    y = df_final[target_col]\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=None\n",
    "    )\n",
    "    \n",
    "    print(f\"🔄 Train-Test Split:\")\n",
    "    print(f\"   Training set: {X_train.shape[0]:,} samples\")\n",
    "    print(f\"   Test set: {X_test.shape[0]:,} samples\")\n",
    "    print(f\"   Features: {X_train.shape[1]}\")\n",
    "    \n",
    "    # Check target distribution\n",
    "    print(f\"\\n📊 Target distribution (fatalities):\")\n",
    "    print(f\"   Train mean: {y_train.mean():.2f} ± {y_train.std():.2f}\")\n",
    "    print(f\"   Test mean: {y_test.mean():.2f} ± {y_test.std():.2f}\")\n",
    "    print(f\"   Min: {y.min()}, Max: {y.max()}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# =============================================================================\n",
    "# USAGE EXAMPLE\n",
    "# =============================================================================\n",
    "\n",
    "# Run the complete preprocessing pipeline:\n",
    "df_ready, encoders = prepare_xgboost_features(df)\n",
    "\n",
    "# Prepare for model training:\n",
    "X_train, X_test, y_train, y_test = prepare_train_test_split(df_ready)\n",
    "\n",
    "# Now ready for XGBoost!\n",
    "import xgboost as xgb\n",
    "\n",
    "# Create XGBoost dataset\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Train model\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "model = xgb.train(params, dtrain, num_boost_round=100, \n",
    "                  evals=[(dtrain, 'train'), (dtest, 'test')],\n",
    "                  early_stopping_rounds=10, verbose_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Train-Test Split:\n",
      "   Training set: 7,686 samples\n",
      "   Test set: 1,922 samples\n",
      "   Features: 33\n",
      "\n",
      "📊 Target distribution (fatalities):\n",
      "   Train mean: 5.10 ± 10.99\n",
      "   Test mean: 5.32 ± 11.59\n",
      "   Min: 1, Max: 213\n",
      "🔍 Starting Quick Hyperparameter Tuning (Randomized Search)\n",
      "------------------------------------------------------------\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "⏱️ Training completed in 31.0 seconds\n",
      "🎯 Best CV Score (RMSE): 6.205\n",
      "\n",
      "📊 Quick Tuned XGBoost Performance:\n",
      "   RMSE: 6.370\n",
      "   MAE:  2.464\n",
      "   R²:   0.698\n",
      "   MAPE: 80.95%\n",
      "   Mean residual: 0.294\n",
      "   Std residuals: 6.365\n",
      "\n",
      "🏆 Best Parameters:\n",
      "   subsample: 1.0\n",
      "   reg_lambda: 1\n",
      "   reg_alpha: 1\n",
      "   n_estimators: 100\n",
      "   min_child_weight: 7\n",
      "   max_depth: 3\n",
      "   learning_rate: 0.15\n",
      "   gamma: 0.5\n",
      "   colsample_bytree: 0.7\n",
      "   colsample_bylevel: 1.0\n",
      "🏁 COMPREHENSIVE HYPERPARAMETER TUNING COMPARISON\n",
      "================================================================================\n",
      "\n",
      "==================================================\n",
      "METHOD 1: QUICK RANDOM SEARCH\n",
      "==================================================\n",
      "🔍 Starting Quick Hyperparameter Tuning (Randomized Search)\n",
      "------------------------------------------------------------\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "⏱️ Training completed in 27.8 seconds\n",
      "🎯 Best CV Score (RMSE): 6.205\n",
      "\n",
      "📊 Quick Tuned XGBoost Performance:\n",
      "   RMSE: 6.370\n",
      "   MAE:  2.464\n",
      "   R²:   0.698\n",
      "   MAPE: 80.95%\n",
      "   Mean residual: 0.294\n",
      "   Std residuals: 6.365\n",
      "\n",
      "🏆 Best Parameters:\n",
      "   subsample: 1.0\n",
      "   reg_lambda: 1\n",
      "   reg_alpha: 1\n",
      "   n_estimators: 100\n",
      "   min_child_weight: 7\n",
      "   max_depth: 3\n",
      "   learning_rate: 0.15\n",
      "   gamma: 0.5\n",
      "   colsample_bytree: 0.7\n",
      "   colsample_bylevel: 1.0\n",
      "\n",
      "==================================================\n",
      "METHOD 2: ADVANCED SEQUENTIAL GRID SEARCH\n",
      "==================================================\n",
      "🚀 Starting Advanced Hyperparameter Tuning (Sequential Grid Search)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "1️⃣ Tuning n_estimators and learning_rate...\n",
      "   Best RMSE: 6.170\n",
      "   Best params: {'learning_rate': 0.03, 'n_estimators': 200}\n",
      "\n",
      "2️⃣ Tuning tree structure (max_depth, min_child_weight)...\n",
      "   Best RMSE: 6.170\n",
      "   Best params: {'max_depth': 3, 'min_child_weight': 7}\n",
      "\n",
      "3️⃣ Tuning sampling (subsample, colsample_bytree)...\n",
      "   Best RMSE: 6.169\n",
      "   Best params: {'colsample_bylevel': 0.6, 'colsample_bytree': 0.9, 'subsample': 1.0}\n",
      "\n",
      "4️⃣ Tuning regularization (gamma, reg_alpha, reg_lambda)...\n",
      "   Best RMSE: 6.167\n",
      "   Best params: {'gamma': 0.2, 'reg_alpha': 1, 'reg_lambda': 1}\n",
      "\n",
      "🎯 ADVANCED TUNING COMPLETE!\n",
      "   Best CV RMSE: 6.167\n",
      "\n",
      "📊 Advanced Tuned XGBoost Performance:\n",
      "   RMSE: 6.270\n",
      "   MAE:  2.509\n",
      "   R²:   0.707\n",
      "   MAPE: 83.90%\n",
      "   Mean residual: 0.136\n",
      "   Std residuals: 6.270\n",
      "\n",
      "📈 Tuning Progression:\n",
      "   n_estimators + learning_rate: 6.170\n",
      "   Tree structure: 6.170\n",
      "   Sampling: 6.169\n",
      "   Regularization: 6.167\n",
      "\n",
      "🏆 Final Optimized Parameters:\n",
      "   base_score: None\n",
      "   booster: None\n",
      "   callbacks: None\n",
      "   colsample_bylevel: 0.6\n",
      "   colsample_bynode: None\n",
      "   colsample_bytree: 0.9\n",
      "   device: None\n",
      "   early_stopping_rounds: None\n",
      "   enable_categorical: False\n",
      "   eval_metric: None\n",
      "   feature_types: None\n",
      "   feature_weights: None\n",
      "   gamma: 0.2\n",
      "   grow_policy: None\n",
      "   importance_type: None\n",
      "   interaction_constraints: None\n",
      "   learning_rate: 0.03\n",
      "   max_bin: None\n",
      "   max_cat_threshold: None\n",
      "   max_cat_to_onehot: None\n",
      "   max_delta_step: None\n",
      "   max_depth: 3\n",
      "   max_leaves: None\n",
      "   min_child_weight: 7\n",
      "   missing: nan\n",
      "   monotone_constraints: None\n",
      "   multi_strategy: None\n",
      "   n_estimators: 200\n",
      "   num_parallel_tree: None\n",
      "   reg_alpha: 1\n",
      "   reg_lambda: 1\n",
      "   sampling_method: None\n",
      "   scale_pos_weight: None\n",
      "   subsample: 1.0\n",
      "   tree_method: None\n",
      "   validate_parameters: None\n",
      "   verbosity: None\n",
      "\n",
      "==================================================\n",
      "METHOD 3: BAYESIAN OPTIMIZATION\n",
      "==================================================\n",
      "❌ Optuna not installed. Install with: pip install optuna\n",
      "   Falling back to advanced grid search...\n",
      "🚀 Starting Advanced Hyperparameter Tuning (Sequential Grid Search)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "1️⃣ Tuning n_estimators and learning_rate...\n",
      "   Best RMSE: 6.551\n",
      "   Best params: {'learning_rate': 0.01, 'n_estimators': 200}\n",
      "\n",
      "2️⃣ Tuning tree structure (max_depth, min_child_weight)...\n",
      "   Best RMSE: 6.260\n",
      "   Best params: {'max_depth': 8, 'min_child_weight': 10}\n",
      "\n",
      "3️⃣ Tuning sampling (subsample, colsample_bytree)...\n",
      "   Best RMSE: 6.251\n",
      "   Best params: {'colsample_bylevel': 1.0, 'colsample_bytree': 0.8, 'subsample': 1.0}\n",
      "\n",
      "4️⃣ Tuning regularization (gamma, reg_alpha, reg_lambda)...\n",
      "   Best RMSE: 6.247\n",
      "   Best params: {'gamma': 0.3, 'reg_alpha': 1, 'reg_lambda': 1}\n",
      "\n",
      "🎯 ADVANCED TUNING COMPLETE!\n",
      "   Best CV RMSE: 6.247\n",
      "\n",
      "📊 Advanced Tuned XGBoost Performance:\n",
      "   RMSE: 6.448\n",
      "   MAE:  2.510\n",
      "   R²:   0.690\n",
      "   MAPE: 87.99%\n",
      "   Mean residual: 0.173\n",
      "   Std residuals: 6.447\n",
      "\n",
      "📈 Tuning Progression:\n",
      "   n_estimators + learning_rate: 6.551\n",
      "   Tree structure: 6.260\n",
      "   Sampling: 6.251\n",
      "   Regularization: 6.247\n",
      "\n",
      "🏆 Final Optimized Parameters:\n",
      "   base_score: None\n",
      "   booster: None\n",
      "   callbacks: None\n",
      "   colsample_bylevel: 1.0\n",
      "   colsample_bynode: None\n",
      "   colsample_bytree: 0.8\n",
      "   device: None\n",
      "   early_stopping_rounds: None\n",
      "   enable_categorical: False\n",
      "   eval_metric: None\n",
      "   feature_types: None\n",
      "   feature_weights: None\n",
      "   gamma: 0.3\n",
      "   grow_policy: None\n",
      "   importance_type: None\n",
      "   interaction_constraints: None\n",
      "   learning_rate: 0.01\n",
      "   max_bin: None\n",
      "   max_cat_threshold: None\n",
      "   max_cat_to_onehot: None\n",
      "   max_delta_step: None\n",
      "   max_depth: 8\n",
      "   max_leaves: None\n",
      "   min_child_weight: 10\n",
      "   missing: nan\n",
      "   monotone_constraints: None\n",
      "   multi_strategy: None\n",
      "   n_estimators: 200\n",
      "   num_parallel_tree: None\n",
      "   reg_alpha: 1\n",
      "   reg_lambda: 1\n",
      "   sampling_method: None\n",
      "   scale_pos_weight: None\n",
      "   subsample: 1.0\n",
      "   tree_method: None\n",
      "   validate_parameters: None\n",
      "   verbosity: None\n",
      "Bayesian optimization failed: too many values to unpack (expected 3)\n",
      "\n",
      "================================================================================\n",
      "🏆 FINAL COMPARISON\n",
      "================================================================================\n",
      "                      Test RMSE  Test MAE  Test R²  Time (min)\n",
      "Quick Random Search        6.37     2.464    0.698       0.463\n",
      "Advanced Grid Search       6.27     2.509    0.707       0.828\n",
      "\n",
      "🥇 WINNER: Advanced Grid Search\n",
      "   Best Test RMSE: 6.270\n",
      "   Training time: 0.8 minutes\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def evaluate_model_performance(model, X_test, y_test, model_name=\"Model\"):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    \n",
    "    # Make predictions\n",
    "    if hasattr(model, 'predict'):\n",
    "        y_pred = model.predict(X_test)\n",
    "    else:  # XGBoost DMatrix case\n",
    "        dtest = xgb.DMatrix(X_test)\n",
    "        y_pred = model.predict(dtest)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Additional metrics\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / np.maximum(y_test, 1))) * 100  # Avoid division by zero\n",
    "    \n",
    "    print(f\"\\n📊 {model_name} Performance:\")\n",
    "    print(f\"   RMSE: {rmse:.3f}\")\n",
    "    print(f\"   MAE:  {mae:.3f}\")\n",
    "    print(f\"   R²:   {r2:.3f}\")\n",
    "    print(f\"   MAPE: {mape:.2f}%\")\n",
    "    \n",
    "    # Residual analysis\n",
    "    residuals = y_test - y_pred\n",
    "    print(f\"   Mean residual: {residuals.mean():.3f}\")\n",
    "    print(f\"   Std residuals: {residuals.std():.3f}\")\n",
    "    \n",
    "    return {'rmse': rmse, 'mae': mae, 'r2': r2, 'mape': mape}\n",
    "\n",
    "def quick_hyperparameter_tuning(X_train, y_train, X_test, y_test, n_trials=50):\n",
    "    \"\"\"\n",
    "    Fast randomized search for hyperparameter tuning\n",
    "    \"\"\"\n",
    "    print(\"🔍 Starting Quick Hyperparameter Tuning (Randomized Search)\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Define parameter distribution for randomized search\n",
    "    param_dist = {\n",
    "        'n_estimators': [100, 200, 300, 500, 800, 1000],\n",
    "        'max_depth': [3, 4, 5, 6, 7, 8, 10],\n",
    "        'learning_rate': [0.01, 0.03, 0.05, 0.1, 0.15, 0.2, 0.3],\n",
    "        'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'colsample_bylevel': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'min_child_weight': [1, 3, 5, 7, 10],\n",
    "        'gamma': [0, 0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "        'reg_alpha': [0, 0.001, 0.01, 0.1, 1, 10],\n",
    "        'reg_lambda': [0, 0.001, 0.01, 0.1, 1, 10]\n",
    "    }\n",
    "    \n",
    "    # XGBoost regressor\n",
    "    xgb_model = xgb.XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Randomized search\n",
    "    start_time = time.time()\n",
    "    random_search = RandomizedSearchCV(\n",
    "        xgb_model,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=n_trials,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        cv=5,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit\n",
    "    random_search.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"⏱️ Training completed in {training_time:.1f} seconds\")\n",
    "    print(f\"🎯 Best CV Score (RMSE): {-random_search.best_score_:.3f}\")\n",
    "    \n",
    "    # Test set evaluation\n",
    "    best_model = random_search.best_estimator_\n",
    "    test_metrics = evaluate_model_performance(best_model, X_test, y_test, \"Quick Tuned XGBoost\")\n",
    "    \n",
    "    print(f\"\\n🏆 Best Parameters:\")\n",
    "    for param, value in random_search.best_params_.items():\n",
    "        print(f\"   {param}: {value}\")\n",
    "    \n",
    "    return best_model, random_search.best_params_, test_metrics\n",
    "\n",
    "def advanced_hyperparameter_tuning(X_train, y_train, X_test, y_test, base_params=None):\n",
    "    \"\"\"\n",
    "    Advanced sequential hyperparameter tuning with GridSearchCV\n",
    "    \"\"\"\n",
    "    print(\"🚀 Starting Advanced Hyperparameter Tuning (Sequential Grid Search)\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    if base_params is None:\n",
    "        # Start with reasonable defaults\n",
    "        current_params = {\n",
    "            'objective': 'reg:squarederror',\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "    else:\n",
    "        current_params = base_params.copy()\n",
    "    \n",
    "    best_score = float('inf')\n",
    "    best_model = None\n",
    "    tuning_history = []\n",
    "    \n",
    "    # Step 1: Tune n_estimators and learning_rate\n",
    "    print(\"\\n1️⃣ Tuning n_estimators and learning_rate...\")\n",
    "    param_grid_1 = {\n",
    "        'n_estimators': [200, 500, 800, 1000, 1500],\n",
    "        'learning_rate': [0.01, 0.03, 0.05, 0.1, 0.15]\n",
    "    }\n",
    "    \n",
    "    grid_search_1 = GridSearchCV(\n",
    "        xgb.XGBRegressor(**current_params),\n",
    "        param_grid_1,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        cv=5,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid_search_1.fit(X_train, y_train)\n",
    "    \n",
    "    current_params.update(grid_search_1.best_params_)\n",
    "    current_score = -grid_search_1.best_score_\n",
    "    print(f\"   Best RMSE: {current_score:.3f}\")\n",
    "    print(f\"   Best params: {grid_search_1.best_params_}\")\n",
    "    \n",
    "    if current_score < best_score:\n",
    "        best_score = current_score\n",
    "        best_model = grid_search_1.best_estimator_\n",
    "    \n",
    "    tuning_history.append(('n_estimators + learning_rate', current_score, grid_search_1.best_params_))\n",
    "    \n",
    "    # Step 2: Tune tree structure\n",
    "    print(\"\\n2️⃣ Tuning tree structure (max_depth, min_child_weight)...\")\n",
    "    param_grid_2 = {\n",
    "        'max_depth': [3, 4, 5, 6, 7, 8],\n",
    "        'min_child_weight': [1, 3, 5, 7, 10]\n",
    "    }\n",
    "    \n",
    "    grid_search_2 = GridSearchCV(\n",
    "        xgb.XGBRegressor(**current_params),\n",
    "        param_grid_2,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        cv=5,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid_search_2.fit(X_train, y_train)\n",
    "    \n",
    "    current_params.update(grid_search_2.best_params_)\n",
    "    current_score = -grid_search_2.best_score_\n",
    "    print(f\"   Best RMSE: {current_score:.3f}\")\n",
    "    print(f\"   Best params: {grid_search_2.best_params_}\")\n",
    "    \n",
    "    if current_score < best_score:\n",
    "        best_score = current_score\n",
    "        best_model = grid_search_2.best_estimator_\n",
    "    \n",
    "    tuning_history.append(('Tree structure', current_score, grid_search_2.best_params_))\n",
    "    \n",
    "    # Step 3: Tune sampling parameters\n",
    "    print(\"\\n3️⃣ Tuning sampling (subsample, colsample_bytree)...\")\n",
    "    param_grid_3 = {\n",
    "        'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'colsample_bylevel': [0.6, 0.8, 1.0]\n",
    "    }\n",
    "    \n",
    "    grid_search_3 = GridSearchCV(\n",
    "        xgb.XGBRegressor(**current_params),\n",
    "        param_grid_3,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        cv=5,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid_search_3.fit(X_train, y_train)\n",
    "    \n",
    "    current_params.update(grid_search_3.best_params_)\n",
    "    current_score = -grid_search_3.best_score_\n",
    "    print(f\"   Best RMSE: {current_score:.3f}\")\n",
    "    print(f\"   Best params: {grid_search_3.best_params_}\")\n",
    "    \n",
    "    if current_score < best_score:\n",
    "        best_score = current_score\n",
    "        best_model = grid_search_3.best_estimator_\n",
    "    \n",
    "    tuning_history.append(('Sampling', current_score, grid_search_3.best_params_))\n",
    "    \n",
    "    # Step 4: Tune regularization\n",
    "    print(\"\\n4️⃣ Tuning regularization (gamma, reg_alpha, reg_lambda)...\")\n",
    "    param_grid_4 = {\n",
    "        'gamma': [0, 0.1, 0.2, 0.3, 0.5],\n",
    "        'reg_alpha': [0, 0.001, 0.01, 0.1, 1],\n",
    "        'reg_lambda': [0, 0.001, 0.01, 0.1, 1]\n",
    "    }\n",
    "    \n",
    "    grid_search_4 = GridSearchCV(\n",
    "        xgb.XGBRegressor(**current_params),\n",
    "        param_grid_4,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        cv=5,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid_search_4.fit(X_train, y_train)\n",
    "    \n",
    "    current_params.update(grid_search_4.best_params_)\n",
    "    current_score = -grid_search_4.best_score_\n",
    "    print(f\"   Best RMSE: {current_score:.3f}\")\n",
    "    print(f\"   Best params: {grid_search_4.best_params_}\")\n",
    "    \n",
    "    if current_score < best_score:\n",
    "        best_score = current_score\n",
    "        best_model = grid_search_4.best_estimator_\n",
    "    \n",
    "    tuning_history.append(('Regularization', current_score, grid_search_4.best_params_))\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(f\"\\n🎯 ADVANCED TUNING COMPLETE!\")\n",
    "    print(f\"   Best CV RMSE: {best_score:.3f}\")\n",
    "    \n",
    "    final_metrics = evaluate_model_performance(best_model, X_test, y_test, \"Advanced Tuned XGBoost\")\n",
    "    \n",
    "    # Show tuning progression\n",
    "    print(f\"\\n📈 Tuning Progression:\")\n",
    "    for step, score, params in tuning_history:\n",
    "        print(f\"   {step}: {score:.3f}\")\n",
    "    \n",
    "    # Final parameters\n",
    "    print(f\"\\n🏆 Final Optimized Parameters:\")\n",
    "    final_params = best_model.get_params()\n",
    "    for param, value in sorted(final_params.items()):\n",
    "        if param not in ['n_jobs', 'random_state', 'objective']:\n",
    "            print(f\"   {param}: {value}\")\n",
    "    \n",
    "    return best_model, current_params, final_metrics, tuning_history\n",
    "\n",
    "def bayesian_optimization_tuning(X_train, y_train, X_test, y_test, n_trials=100):\n",
    "    \"\"\"\n",
    "    Bayesian optimization using Optuna (if available)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import optuna\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        print(\"🧠 Starting Bayesian Optimization with Optuna\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        def objective(trial):\n",
    "            # Define hyperparameter search space\n",
    "            params = {\n",
    "                'objective': 'reg:squarederror',\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 2000),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "                'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.6, 1.0),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),\n",
    "                'gamma': trial.suggest_float('gamma', 0, 1.0),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 0.0001, 100, log=True),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 0.0001, 100, log=True),\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "            \n",
    "            # Cross-validation\n",
    "            model = xgb.XGBRegressor(**params)\n",
    "            scores = cross_val_score(model, X_train, y_train, \n",
    "                                   scoring='neg_root_mean_squared_error', cv=5)\n",
    "            return -scores.mean()  # Optuna minimizes\n",
    "        \n",
    "        # Create study and optimize\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(objective, n_trials=n_trials)\n",
    "        \n",
    "        # Train best model\n",
    "        best_params = study.best_params.copy()\n",
    "        best_params.update({'objective': 'reg:squarederror', 'random_state': 42, 'n_jobs': -1})\n",
    "        \n",
    "        best_model = xgb.XGBRegressor(**best_params)\n",
    "        best_model.fit(X_train, y_train)\n",
    "        \n",
    "        print(f\"🎯 Best CV Score (RMSE): {study.best_value:.3f}\")\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        final_metrics = evaluate_model_performance(best_model, X_test, y_test, \"Bayesian Optimized XGBoost\")\n",
    "        \n",
    "        print(f\"\\n🏆 Best Parameters (Bayesian Optimization):\")\n",
    "        for param, value in study.best_params.items():\n",
    "            print(f\"   {param}: {value}\")\n",
    "        \n",
    "        return best_model, best_params, final_metrics\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"❌ Optuna not installed. Install with: pip install optuna\")\n",
    "        print(\"   Falling back to advanced grid search...\")\n",
    "        return advanced_hyperparameter_tuning(X_train, y_train, X_test, y_test)\n",
    "\n",
    "def compare_tuning_methods(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Compare all tuning methods and select the best one\n",
    "    \"\"\"\n",
    "    print(\"🏁 COMPREHENSIVE HYPERPARAMETER TUNING COMPARISON\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Method 1: Quick Random Search\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"METHOD 1: QUICK RANDOM SEARCH\")\n",
    "    print(\"=\"*50)\n",
    "    start_time = time.time()\n",
    "    model_1, params_1, metrics_1 = quick_hyperparameter_tuning(X_train, y_train, X_test, y_test)\n",
    "    time_1 = time.time() - start_time\n",
    "    results['Quick Random Search'] = {\n",
    "        'model': model_1, 'params': params_1, 'metrics': metrics_1, 'time': time_1\n",
    "    }\n",
    "    \n",
    "    # Method 2: Advanced Sequential Grid Search\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"METHOD 2: ADVANCED SEQUENTIAL GRID SEARCH\")\n",
    "    print(\"=\"*50)\n",
    "    start_time = time.time()\n",
    "    model_2, params_2, metrics_2, history_2 = advanced_hyperparameter_tuning(\n",
    "        X_train, y_train, X_test, y_test, base_params=params_1\n",
    "    )\n",
    "    time_2 = time.time() - start_time\n",
    "    results['Advanced Grid Search'] = {\n",
    "        'model': model_2, 'params': params_2, 'metrics': metrics_2, 'time': time_2\n",
    "    }\n",
    "    \n",
    "    # Method 3: Bayesian Optimization (if available)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"METHOD 3: BAYESIAN OPTIMIZATION\")\n",
    "    print(\"=\"*50)\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        model_3, params_3, metrics_3 = bayesian_optimization_tuning(X_train, y_train, X_test, y_test)\n",
    "        time_3 = time.time() - start_time\n",
    "        results['Bayesian Optimization'] = {\n",
    "            'model': model_3, 'params': params_3, 'metrics': metrics_3, 'time': time_3\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Bayesian optimization failed: {e}\")\n",
    "        time_3 = 0\n",
    "    \n",
    "    # Final comparison\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"🏆 FINAL COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    comparison_df = pd.DataFrame({\n",
    "        method: {\n",
    "            'Test RMSE': data['metrics']['rmse'],\n",
    "            'Test MAE': data['metrics']['mae'],\n",
    "            'Test R²': data['metrics']['r2'],\n",
    "            'Time (min)': data['time'] / 60\n",
    "        }\n",
    "        for method, data in results.items()\n",
    "    }).T\n",
    "    \n",
    "    print(comparison_df.round(3))\n",
    "    \n",
    "    # Select best model\n",
    "    best_method = comparison_df['Test RMSE'].idxmin()\n",
    "    best_model = results[best_method]['model']\n",
    "    \n",
    "    print(f\"\\n🥇 WINNER: {best_method}\")\n",
    "    print(f\"   Best Test RMSE: {results[best_method]['metrics']['rmse']:.3f}\")\n",
    "    print(f\"   Training time: {results[best_method]['time']/60:.1f} minutes\")\n",
    "    \n",
    "    return best_model, results[best_method]['params'], results\n",
    "\n",
    "# =============================================================================\n",
    "# USAGE EXAMPLE\n",
    "# =============================================================================\n",
    "\n",
    "# After running your preprocessing pipeline:\n",
    "X_train, X_test, y_train, y_test = prepare_train_test_split(df_ready)\n",
    "\n",
    "# Option 1: Quick tuning (5-10 minutes)\n",
    "best_model_quick, best_params_quick, metrics_quick = quick_hyperparameter_tuning(\n",
    "    X_train, y_train, X_test, y_test, n_trials=50\n",
    ")\n",
    "\n",
    "# Option 2: Comprehensive comparison (30-60 minutes)\n",
    "best_model, best_params, all_results = compare_tuning_methods(\n",
    "    X_train, y_train, X_test, y_test\n",
    ")\n",
    "\n",
    "# Use the best model for predictions\n",
    "final_predictions = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acled",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
